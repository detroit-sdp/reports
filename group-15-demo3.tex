%% Template for SDP report, adapted from mlp_cw2_template, 2018. 

%% Based on  LaTeX template for ICML 2017 - example_paper.tex at 
%%  https://2017.icml.cc/Conferences/2017/StyleAuthorInstructions

\documentclass{article}
\input{mlp2018_includes}

%% You probably do not need to change anything above this comment

%% REPLACE the details in the following commands with your details
\setGroupNumber{15}
\setGroupName{Detroit}
\setProductName{Tadashi}
\setDemoNumber{3}
\setLogoFileName{figs/logo-small.png}

\begin{document} 

\makeSDPTitle{Demo}

% Previous MLP Style Title Layout working. 
% \twocolumn[
    % \mlptitle{\productName: SDP Demo \demoNumber}
    % \centerline{Group \groupNumber: \groupName}
% ]

\begin{abstract}
  Tadashi is an assistive robot for delivering food and water to residents in supported living, care home, and quarantine environments.
  In this demo, we demonstrate significant progress towards a minimum viable product (MVP). We integrate components of our system together, demonstrating significant progress towards a full use case: the caregiver placing a delivery on the tray and scheduling the robot to visit a room; the robot navigating to the room; the lift extending on arrival and detecting that the resident has picked up the delivery using integrated weight sensors; the lift retracting once the delivery has been made, and the robot sending a message to the app to indicate this; and the robot returning to its starting point. We also discuss usability testing we have performed on our app to ensure it is usable by non-technical caregivers, and discuss future improvements to the app based on this. 
\end{abstract} 

\section{Project plan update} 
\begin{itemize}
\item Begin testing app with non-technical users to gain insights on accessibility and ease of use: {\bf achieved}.
\item Make changes to app justified against results of testing: {\bf partially achieved}.
\item Complete user interviews with caregivers to assess demand for project features and implement new features based on caregiver requests: {\bf not achieved}.
\item Physically integrate lift mechanism into robot body, and integrate lift control into robot control software: {\bf partially achieved}.
\item Design and manufacture a tray to place on top of the lift to hold items: {\bf achieved}.
\item Add sensor to lift to determine whether items have been removed: {\bf partially achieved}.
\item Refine robot problem-solving when stuck by tuning mapping sensitivity and obstacle avoidance logic: {\bf partially achieved}. 
\item Implement bi-directional communication between the app and robot: {\bf achieved}.
\end{itemize}

We were not able to complete user interviews due to delays in receiving ethical approval. We intend to complete these as soon as we receive the approval. 

\begin{table}[]
  \begin{tabular}{l|l}
    Task & Members \\
    \hline
    Robot refinements & Ben, Wojtek \\
    App refinements & Ching Ling, Yuchen \\
    Project management & Michael \\
    User testing & Theo \\
    Product development & Rebecca \\
    Lift building and mounting & Luukas \\
    Lift-robot connection & Errikos, Luukas, Jakub \\
    App-robot connection & Ben, Yuchen \\
  \end{tabular}
  \caption{Task splits across the group up to demo \demoNumber. Core teams and points of contact remain the same as with demo 2, but tasks were done by combining expertise across teams. }
  \label{tab:group-split}
\end{table}

We began the sprint with teams working as for demo 2 to refine individual components, then created ad-hoc teams to connect systems (table \ref{tab:group-split}). We used pair programming to utilize each person's knowledge and reduce the need to merge code. We used GitHub for version control, and Slack and once-weekly meetings for communication.

Our total budget spending so far is shown in section \ref{budget}.

Owing to the ongoing coronavirus situation, the entire team began working remotely from Saturday 14th March. This meant we needed to halt work on the robot and the lift, as we could not access or run these remotely, so goals relating to these parts of the system could only be partially completed. Our original goals for demo 4 are given in the appendix, section \ref{goals}. Given the changing situation, our next goals are as follows:
\begin{itemize}
  \item Complete user interviews, analyze results, and implement changes to our product to match user demand.
  \item Improve app based on usability testing, and perform another round of usability tests to assess if changes have made a difference to usability. 
  \item Write the user guide. 
  \item Complete the website. 
\end{itemize}

\section{Technical details}
\subsection{Connectivity}
To aid the reader with understanding how the system fits together, we provide two diagrams. Figure \ref{fig:seq} is a sequence diagram showing how each part of the system is involved in the use case mentioned in the abstract. Figure \ref{fig:connect-larger} shows the connections we have set up between each of the parts. 

\begin{figure}
  \begin{center}
    \includegraphics[width=\columnwidth]{figs/sequence.png}
  \end{center}
  \caption{A simplified view of the steps of our use case and how each part of the product is involved in the use case. Note that for simplicity, `Tadashi' represents both the Turtlebot and the ROS master.}
  \label{fig:seq}
\end{figure}

\subsubsection{Connectivity between Android app and robot}
\label{approbot}
In the previous demo we only demonstrated sending packets from the Android app to the robot. Implementing the other direction was difficult because the app runs on an emulator. By introducing port forwarding we have been able to demonstrate sending packets in both directions. The host machine running the emulator receives the packets on a port and forwards them to another port on itself. This second port is setup through the emulation software to forward all traffic onto a port on the emulated device.

To facilitate the connection between the app and the robot, the app now has a network service running in the background until the app closes. We want to have the service run at all times, but Android power saving optimizations will eventually destroy the service. The service may have to be moved to the foreground to avoid this. 

To communicate the state of the robot to the app, the robot stores the following information internally. Firstly, {\bf low battery}, which is set if the battery level gets below 15\%. This state requires the caregiver to charge the robot. {\bf Assistance required} is set if the robot cannot find a path to the goal. This state requires rescue from the caregiver. The other possible states are {\bf at base and not busy}, {\bf moving}, and {\bf arrived at goal}. The robot sends a packet containing one of the statuses above: \{low battery, assistance required, at base and not busy, moving, arrived at goal\} when one of these statuses is set. 

\subsubsection{Connectivity between robot and lift}
The Turtlebot can send a command to extend or retract the lift to the Arduino controlling the lift. In return, the lift sends three bytes to the Turtlebot. The first two bytes are the voltage across the weight sensors, divided by 4 to fit 8 bits. The last byte is three zeroes followed by the lift's state of movement, one of \{overloaded, retracted, retracting, extended, extending\}, represented by one-hot encoding (for example, `overloaded' would be represented as {\tt 00010000}, `retracted' as {\tt 00001000}). 

To establish a connection between the Turtlebot and the Arduino controlling the lift, we reviewed multiple options (Bluetooth, WiFi). We decided that the fastest and most reliable method would be to use a serial connection between USB ports on each board.

We wrote scripts and saved them on the Ardunio to extend and retract the lift. To establish connectivity, we wrote Python scripts that could read/write bytes of data over the cable to communicate with the Ardunio. The Raspberry Pi sends a single byte to the Arduino with possible values \{0, 1, 2\} indicating stop, extend lift, and retract lift respectively. The Arduino returns the status of the lift as well as the current readings from the two weight sensors.

The connection was initially unreliable due to synchronization issues. By sending 0-bytes to the Arduino we can determine if the connection is disconnected, or just delayed. Details of this are in the appendix, section \ref{connection}.

For safety reasons, if the connection between the Turtlebot and the Arduino is broken, the lift stops all operation. If the connection is not re-established within 20 seconds, the lift lowers itself down.  

\subsection{Hardware}
We attached the lift to the Turtlebot using MDF and mounting screws drilled into the Lego (figure \ref{fig:lift}). The lift is stable when the robot is moving. We also designed and manufactured a tray to place on top of the lift, indicating where items are to be placed (figure \ref{fig:sensor}). The figure also shows one of the weight sensors. We placed two weight sensors under the tray: these can be used to detect when deliveries have been placed and picked up on the robot. The weight sensors send voltage values to the Arduino, which are sent forward to the Turtlebot for interpretation. We intended to do testing to determine how best to interpret the weight sensor values but were not able to do this before the team moved to remote working. 

We replaced the EV3 brick with an Ardunio (figure \ref{fig:arduino}) as it was easier to set up a serial connection with the Arduino, and it had significantly faster boot times. 


\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\columnwidth]{figs/lift_diag.png}
    \caption{Side view of the lift mounted to the Turtlebot. Points to note include the metal brackets used to fix the lift in place (1); the removal of the EV3 brick and addition of the Arduino kit and batteries (2); and the tray and weight sensor placed on top (3).}
  \label{fig:lift}
  \end{center}
\end{figure}

\subsection{User interface}
\label{ui}
We provide 6 key functionalities that users can perform using the app:
\begin{enumerate}
\item Create an account and sign in. 
\item Add new residents to the rooms of the map.
\item Send Tadashi to deliver to a resident. 
\item Set a delivery for a later date in the calendar. 
\item Report back information contained in the dashboard.
\item Sign out.
\end{enumerate}
We also provide notifications for messages sent from the robot to the app as described in section \ref{approbot}.
\\
We have finalized the four fragments that achieve this functionality using best practices for app design \cite{design, ux}:
\begin{enumerate}
  \item {\bf Dashboard}: for viewing robot/delivery information.
  \item {\bf Map}: shows top-down map of the ward with an icon on each room to add/remove. 
  \item {\bf Calendar}: for scheduling and checking robots itinerary.
  \item {\bf Me}: shows logged-in account general information.
\end{enumerate}

The main focus for this demo was on the calendar fragment, and the networking service. The calendar allows the user to select any date, and for the given date it displays the robot's itinerary (past and future) for that date, in chronological order.

 The events are stored and kept track of in a database within our Firebase setup. The database layout is described in table~\ref{tab:firebase}. The robot status is stored using Firebase's real-time database as it is a constantly updating piece of information. 

\subsection{Software}
The robot moves to a specific point on the map and arrives there within some tolerance. For example, if told to move to $(3, 5)$ with a tolerance of $0.5$, it might stop anywhere within $(2.5 \leq x \leq 3.5, 4.5 \leq y \leq 5.5)$. In demo 2, we issued directions as offsets: for example, we would tell the robot to move $4$ points along the $x$-axis and $5$ along the $y$-axis. This meant that any drift from the goal increased as we sent more commands to the robot. To solve this, we started dynamically calculating new offsets based on the difference between the robot's actual position in the demo space and the target position. This closed loop planning significantly reduces drift over time and ensures the robot can more accurately get to the resident in the target room. 

The robot operates within a space that is representative of the real world, but far smaller. When arriving at its goal, the robot will by default try to face in the direction it was facing when it started moving. However, in the restricted demo space, there may not be enough space for the robot to rotate to face this direction. To solve this we pass the navigation stack a target rotation as well as the goal location. We found that restricting the target rotation to be a multiple of 90 degrees from the initial orientation reduced this problem. For each destination room, in addition to a location we hardcoded a target rotation that ensured the robot did not need to rotate too much to reach a specific orientation. In the real world, where there would be more space to maneuver, this would not be a problem, but it was necessary to introduce this restriction within the demo space.

In initial tests runs of our robot, we found two problems.
Firstly, the robot's tolerance in assessing whether it had successfully arrived at its goal was too low. This meant that the robot would think it could not make it to the goal location, even if it was very close to it. We fixed this by increasing the xy\_goal\_tolerance parameter.
Secondly, the robot was moving too quickly between locations. This caused the lift --- and deliveries on the lift --- to become unstable. We decreased the max\_trans\_vel and max\_vel\_x parameters to force the robot to move slower, which made the lift and its delivery appear more stable.
We also made some minor changes to other parameters to improve the robot's navigation ability. These are described in table \ref{tab:params}.  

In mounting the lift onto the Turtlebot, we increased the length of its footprint from 28.1cm to 40.0cm. We were concerned that this would cause the Turtlebot to drive into obstacles. We altered the <box\_size> parameter within the <collision> tag in the description of itself the robot provides to the navigation stack to reflect the change in size. We were not able to test whether this prevented collisions.


\subsection{Business development}
The target market for Tadashi is care homes and supported living environments. There are approximately 433,000 people occupying care homes in the UK, with 5500 providers \cite{carehomes}. Currently, robots are being designed to take care of the elderly, however in these situations there is very little human interaction between the resident and the caregiver \cite{robotcarers}. Our robot is to be used to aid caregivers, not to take over their jobs.

Another target market we have recently identified for Tadashi is in quarantine situations. Tadashi could be used to deliver food, water, and other supplies to quarantined people without the need for medical staff to expose themselves to possible infection. With over 100 million Europeans in quarantine due to the ongoing threat of the coronavirus pandemic \cite{quarantine}, and people over 70 in the UK likely to be asked to self-isolate in the near future \cite{isolate}, we see a strong demand for assistive robots like Tadashi. A hotel in China has recently introduced a robot that serves food to quarantined patients \cite{peanut}. Tadashi improves on this idea by having a lift, which allows quarantined people with mobility issues to reach their deliveries.

\section{Evaluation}
We were unable to perform software and hardware testing, but our planned tests are described in section \ref{testing}. 
We performed usability testing to ensure the app was usable by non-technical caregivers. We decided to perform our testing in an iterative manner, performing three cycles of testing with app modifications during each cycle. Following discussions with Chris during his office hour, a cycle consists of 5 steps.
Step 1, prepare a testing task for the user to complete.
Step 2, self-evaluate our app against the task to check for clear problems.
Step 3, minor modifications to the task or app based on self-evaluation.
Step 4, conduct user testing.
Step 5, major app modifications based on testing.

For demo \demoNumber\ we performed one cycle of testing:

{\bf Step 1}: We set the user the task of following the 6 key functionality steps listed in section \ref{ui}, with 3 residents being added. Each task is relevant to one of the 10 usability principles \cite{heuristics}, as discussed in section \ref{usability}.

{\bf Step 2}: During the self evaluation stage we completed a cognitive walkthrough of the app, putting us in the mind of a new user and completing our set tasks. The main problem found here was a general lack of direction and advice. Since the user starts at the dashboard, it wasn't immediately obvious that they should then use the map section to start adding residents.

{\bf Step 3}: We added help buttons to the app which display a brief explanation of the current section, to help a user in case they get stuck.

{\bf Step 4}: We tested with three users. Each participant signed a consent form, and then sat down with a team member to complete the tasks. We gave each participant a briefing of the use case. We tried to give the testers as little technical advice as necessary so they could use the app as a real user would. {\bf Two of the users successfully completed all six of the tasks. One failed to complete the third task but completed the rest with guidance.} Some users got confused at task 2 and working out the need to move from the dashboard to the map to add residents.

{\bf Step 5}: Based on the results of this testing, the changes we will make to the app are as follows. Fully integrate the `help' dialogues into the app and perform testing to see if this improves usability and reduces failure rates at task 3. Add a `begin adding residents' button to the dashboard that takes them to the map and see if this makes task 2 easier to complete. Finally, as an additional test, we plan to perform A/B testing to compare different dashboard layouts to see which one shows relevant information more intuitively. 

\section{Budget}
\label{budget}
As with the previous demo, the bulk of our costs are associated with building, mounting, and integrating the lift. Tables \ref{tab:non-budget-cost}-\ref{tab:budget-cost-non-monetary} show spending to date including spending at previous demos. Equipment costs are based on values provided in the SDP wiki \cite{sdpcosts}.

\begin{table}[h]
  \begin{center}
    \begin{small}
      \begin{sc}
    \resizebox{\columnwidth}{!}{
  \begin{tabular}{lllll}
    {\bf Item} & {\bf Units} & {\bf Cost (\pounds\ per unit)} & {\bf Total cost (\pounds)} & {\bf Use} \\
    \hline
    Turtlebot Waffle & 1 & 1275.80 & 1275.80 & Robot \\
    Lego & 2.5kg & 15.00 & 37.50 & Lift \\
    EV3 large motor & 2 & 32.39 &  64.78 & Lift \\
    EV3 touch sensor & 2 & 19.19 & 38.38 & Lift \\
    Arduino kit & 1 & 60.00 & 60.00 & Lift \\
    \hline \hline
               &       & Overall cost (\pounds) & 1476.46
  \end{tabular}}

\end{sc}
\end{small}
\caption{Non-budgeted monetary costs at demo \demoNumber. Note that we have chosen to stop using the EV3 and instead use the Arduino kit.}
\label{tab:non-budget-cost}
\end{center}
\end{table}

\begin{table}[h]
  \begin{center}
    \begin{small}
      \begin{sc}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{lllll}
    {\bf Item} & {\bf Total cost (\pounds)} & {\bf Use} \\
    \hline
    Turtlebot supports  & 8.00 & Lift \\
    MDF support (12mm x 270mm x 400mm) & 1.00  & Lift \\ 
    Lift support rods & 5.00 & Lift \\
    Pegboard ($0.62 \text{m}^2$ at \pounds 12 per $2.88 \text{m}^2$)& 2.59 & Arena \\
    Misc (wires, screws, angle brackets) & 10.00 & Lift, arena \\
    MDF support (6 x 350  x 230mm) & 0.30 & Lift \\
    MDF support (9 x 70 x 230mm, 9 x 10 x 250mm (2x)) & 0.10 & Lift \\
    Engraved tray & 5.80 & Lift \\
    Weight sensors (2x) & 19.60 & Lift \\
    \hline
    \hline
     Budget spent (\pounds) & 52.39 \\
     Budget remaining (\pounds) & 147.61
  \end{tabular}}
\end{sc}
\end{small}

\caption{Budgeted monetary costs at demo \demoNumber.}
\label{tab:budget-cost-monetary}

\end{center}
\end{table}

\begin{table}[h]
  \begin{center}
    \begin{small}
      \begin{sc}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{lll}
    {\bf Purpose} & {\bf Hours spent} & {\bf Use} \\
    \hline
    Manufacturing lift support rods & 1 & Lift \\
    Manufacturing lift base (MDF) & 0.5 & Lift \\
    Drilling holes in lift base & 0.25 & Lift \\
    Engraving tray & 1 & Lift \\
    Attaching lift to base & 1 & Lift \\
    Cutting MDF to mount lift to base & 1 & Lift \\
    \hline
    \hline
    Overall technician hours spent & 4.75 & \\
    Total technician hours remaining & 2.25
  \end{tabular}}
\end{sc}
\end{small}
\caption{Budgeted technician time at demo \demoNumber. Note that hours remaining includes subtracting any hours lost due to not being used in previous weeks.}
\label{tab:budget-cost-non-monetary}
\end{center}
\end{table}


% Clearpage also moves floats
\clearpage
\section{Appendix}
\subsection{Planned goals for demo 4}
\label{goals}
Below are our initially planned goals for demo 4 which we will not be able to complete:
\begin{itemize}
  \item Stop the robot from moving if the lift is extended, for safety reasons.
  \item Stop the lift from moving if it is overloaded (defined as loaded with a weight above 1.5kg).
  \item Test and choose the methodology for interpreting weight sensor reading values. This would involve first understanding how the voltages output map to the weights of objects placed on the sensors. Then we would test different orientations of the sensors under the tray, and decide how to combine the two sensor outputs, eg. by arithmetic or harmonic mean. 
  \item Implement checking of the battery charge on the lift, allowing for notification of both lift and robot battery status.
  \item Finalize and test connections between all components. 
\end{itemize}
\subsection{Software and hardware testing}
\label{testing}
We had planned to complete testing of the robot and lift to ensure it met the use case needs. We have not been able to complete these tests as planned. Below are the three tests we intended to do:

\begin{itemize}
\item To ensure the robot can consistently reach the target location for a delivery and return back to base, we would run the robot 10 times and measure how far it stopped from the goal. We defined the upper accuracy threshold as being 20cm radius away from the intended goal: this is still within reasonable reaching distance for a person collecting a delivery. If our testing found that the robot was not able to consistently stop within 20cm of the goal, ie. achieve this 9 out of 10 times, we planned to undertake further work to investigate and fix the source of this issue. 
\item We would also time how long each delivery took to ensure the robot can deliver food and water in a given amount of time (this is especially relevant if the food is hot). This would allow us to communicate to the caregiver a confidence interval for how long a delivery might take, as well as possibly send a notification to the app if the delivery were taking significantly longer than expected based on these tests.
\item The purpose of the weight sensors is to reliably be able to detect when objects have been placed onto and removed from the tray. We intended to perform tests where we would:
  \begin{itemize}
    \item Measure the weight with only the tray on top as the `empty' state of the lift.
    \item Place a cup, and then a plate, on the lift, measuring the changes in weight to mark the lift as containing a delivery. 
    \item Remove the cup, and then the plate, from the lift, measuring changes in weight to mark the delivery as completed. 
  \end{itemize}
  Based on a test run, we would program the lift control in the Turtlebot to be able to recognize when one or both objects were placed onto or removed from the tray. We would then perform 5 tests of these parameters by ensuring that the Turtlebot could reliably (5/5 times) recognize when both objects were placed on and taken off, or one at a time. 
\end{itemize}

\subsection{Details of connection between robot and lift}
\label{connection}
The serial connection between the Raspberry Pi and the Arduino tends to have a variable response time and can sometimes break, not sending back any data.

As the Ardiuno is expected to always respond to messages from the Pi, we can check if it works by sending the `stop' command (a byte filled with zeroes) and waiting for a response. Usually it comes back within a fraction of a second, but sometimes it can lag, so we wait for up to one second. If no reply comes back, a reconnect function is executed, which repeatedly sends empty bytes every 50ms and checks if the input buffer fills up. When it detects a response, it waits an additional second and then clears the buffer. This way it's very unlikely the buffer fills up with late responses, delaying the connection. After a response is detected, most often it means the connection is working well again.

\subsection{Relevance of UI/UX testing to usability principles}
\label{usability}
\begin{enumerate}
  \item Create an account and sign in. \\{\it Standardization: since most users already have experience with sign-up screens, we used a default template.}
  \item Add 3 new residents to the rooms of the map. \\{\it Matching between the system and the real world: since the map represents a real care home floor, we want the user to intuitively recognize the layout, so the map is in the layout of the care home.}
  \item Send Tadashi to deliver to a resident. \\{\it User control: we want the user to feel in control of the robot, so we provide feedback and undo options.}
  \item Set a delivery for a later date in the calendar. \\{\it Standardization: the look of the scheduling system mimics Outlook, which users will be familiar with.}
  \item Report back information contained in the dashboard. \\{\it Visibility of system status: the dashboard is the most updated feature and contains key information, so it needs to display information relevant to the user in an intuitive manner.}
  \item Sign out. \\{\it Minimalism: we keep the settings area clear so users aren't confused on where to sign out.}
\end{enumerate}

\begin{figure}
  \begin{center}
    \includegraphics[width=\columnwidth]{figs/weight_sensor.jpg}
    \caption{The top of the lift, showing the manufactured tray with the Tadashi logo and weight sensor underneath.}
  \label{fig:sensor}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=\columnwidth]{figs/lift_detail.jpg}
    \caption{Inside view of lift mechanics. Note that the motors and sensors are now controlled by an Arduino to better allow for communication with the robot. }
  \label{fig:arduino}
  \end{center}
\end{figure}

\begin{figure*}
  \begin{center}
    \includegraphics[width=1.8\columnwidth]{figs/connectivity.pdf}
  \end{center}
  \caption{The connections between each part of the system, showing what messages are sent during the use case.}
  \label{fig:connect-larger}
\end{figure*}

\begin{table}[h]
\vskip 3mm
\begin{center}
\begin{tabular}{lcc}
\hline
\abovespace\belowspace
Parameters & Original value & Our value \\
\hline
  max\_vel\_x & 0.26 & 0.1 \\
  min\_vel\_x & -0.26 & -0.1 \\
  max\_trans\_vel & 0.26 & 0.1 \\
  min\_trans\_vel & 0.13 & 0.07 \\
  max\_rot\_vel & 1.82 & 1.3 \\
  xy\_goal\_tolerance & 0.05 & 0.175 \\
  path\_distance\_bias & 32.0 & 65.0 \\
  goal\_distance\_bias & 20.0 & 40.0 \\
  occdist\_scale & 0.02 & 0.01
\end{tabular}
\caption{Our parameter alterations. [min/max]\_trans\_vel is the speed of the robot when moving in a straight line. path\_distance\_bias determines how closely the robot need follow the path it works out. goal\_distance\_bias smoothes the trajectory of the path and makes it more efficient. Finally, occdist\_scale determines how `afraid' the robot is of obstacles.}
\label{tab:params}
\end{center}
\vskip -3mm
\end{table}

\begin{table}[h]
  \begin{tabular}{|l|l|}
    \hline
    DeliveryHistory & Residents \\
    \hline
    {\bf location} & {\bf location} \\
    {\it userID} & {\it carer} \\
    deliveryID & firstName \\
    time & lastName \\
    categry & priority \\
    weightBefore & \\
    weightAfter &  \\
    deliverySuccessful & \\
    \hline
  \end{tabular}
  \caption{Database layout in Firebase supporting the Android app. Bold entries represent fields that are linked between the tables, and italic entries represent fields which are linked to Firebase's authentication feature for secure user data storage.}
  \label{tab:firebase}
\end{table}

\clearpage
\bibliography{demo1-refs}


\end{document} 

